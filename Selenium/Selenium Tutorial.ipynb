{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671c09e1-4822-4c81-9a00-d643a3ca2511",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">\n",
    "    SELENIUM TUTORIAL\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff85313-777f-49bd-a2ba-3b8c61b79680",
   "metadata": {},
   "source": [
    "Selenium is a powerful Python library used for automating web browser interactions. It allows users to programmatically navigate websites, fill out forms, click buttons, scrape data, and even simulate user behavior such as scrolling or uploading files. This notebook showcases a series of progressively complex use cases that demonstrate how Selenium can be applied to real-world business and data analytics scenarios. Starting with basic tasks like taking a screenshot and logging into a website, the use cases build up to advanced projects involving infinite scrolling, multi-page scraping, and full automation workflows that include data cleaning and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a4447e-5bb0-44c0-906b-74ceb9ebb4cf",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b834051-cd7d-4599-996a-bdc0c85b442a",
   "metadata": {},
   "source": [
    "This notebook uses a combination of libraries to perform web automation, data extraction, and analysis. Selenium and webdriver_manager are used to automate browser interactions and scrape content from dynamic websites. pandas structures the extracted data, while re and nltk (with stopwords) help clean and process natural language text. collections.Counter is used to count word frequencies. For visualization, matplotlib generates bar charts and manages axis formatting, and wordcloud creates visual representations of word usage. The os and time modules support file handling and script timing. Together, these tools enable an end-to-end workflow for web scraping and content analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "020df502-9de8-43b6-a2f4-b42ddbcfab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c7b271-f7f9-4734-bbc5-9c7d8de0ca17",
   "metadata": {},
   "source": [
    "### Use Case 1: Simple Screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614f760-98e0-4d0d-9fca-5f3ebaafb8cb",
   "metadata": {},
   "source": [
    "This use case demonstrates how to automate the process of capturing a webpage screenshot using Selenium. The script launches a Chrome browser, navigates to the official Selenium website, waits briefly for the page to load, and saves a full-page screenshot to a designated folder. This introduces basic Selenium commands such as launching the browser, navigating to a URL, and using save_screenshot() for visual documentation or testing purposes. While it uses a simple time.sleep() for page loading, future use cases will incorporate more robust waiting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2815f462-a855-475a-80b9-7c45c1bcc877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! A screenshot of the webpage has been saved in the \"Screenshots\" folder.\n"
     ]
    }
   ],
   "source": [
    "# Launch browser (Chrome)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open a webpage\n",
    "driver.get(\"https://www.selenium.dev/\")\n",
    "\n",
    "# Wait for page to load (better to use WebDriverWait in later use cases)\n",
    "time.sleep(2)\n",
    "\n",
    "# Take a screenshot\n",
    "driver.save_screenshot(\"Screenshots/1 - Webpage Screenshot.png\")\n",
    "\n",
    "# Display success message\n",
    "print('âœ… Success! A screenshot of the webpage has been saved in the \"Screenshots\" folder.')\n",
    "\n",
    "# Close browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3373ba-62b1-41fb-8bb4-2919228fb339",
   "metadata": {},
   "source": [
    "### Use Case 2: Account Login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f7c76-4121-417d-96e9-fd8b93bed769",
   "metadata": {},
   "source": [
    "This use case demonstrates how to automate the process of logging into a website using Selenium. The script navigates to a sample login page, waits for the login form to load, enters a predefined username and password, submits the form, and waits for confirmation of a successful login. It concludes by capturing a screenshot of the post-login page. This example introduces form interaction, button clicks, and the use of explicit waits to ensure that elements are fully loaded before interacting with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b03bbee-5b5e-43b3-b45d-752d2d7256ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! A screenshot of the webpage after logging in has been saved in the \"Screenshots\" folder.\n"
     ]
    }
   ],
   "source": [
    "# Launch browser (Chrome)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to the login page\n",
    "driver.get(\"https://the-internet.herokuapp.com/login\")\n",
    "\n",
    "# Wait for the login form to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"username\")))\n",
    "\n",
    "# Fill in username and password\n",
    "driver.find_element(By.ID, \"username\").send_keys(\"tomsmith\")\n",
    "driver.find_element(By.ID, \"password\").send_keys(\"SuperSecretPassword!\")\n",
    "\n",
    "# Click the login button\n",
    "driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\").click()\n",
    "\n",
    "# Wait for the login to complete and dashboard to appear\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"flash.success\")))\n",
    "\n",
    "# Save screenshot\n",
    "driver.save_screenshot(\"Screenshots/2 - Account Login.png\")\n",
    "\n",
    "# Display success message\n",
    "print('âœ… Success! A screenshot of the webpage after logging in has been saved in the \"Screenshots\" folder.')\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94af905-118b-481a-8f66-308494b29528",
   "metadata": {},
   "source": [
    "### Use Case 3: Web Form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d22056-cddc-49ca-8030-44b20d3075ec",
   "metadata": {},
   "source": [
    "This use case demonstrates how to automate filling out and submitting a web form using Selenium. The script inputs text into various fields, selects options from dropdown menus, checks and unchecks checkboxes and radio buttons, picks a color and date, adjusts a range slider using JavaScript, and uploads a local PDF file. It captures screenshots both before and after form submission for verification. This example highlights Seleniumâ€™s ability to handle different types of form elements and introduces basic DOM interaction, condition checking, and JavaScript execution for complex controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d352c40-e4cc-491f-97cf-c36bbb01fee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! Screenshots of the completed web form and the confirmation page have been saved in the \"Screenshots\" folder.\n"
     ]
    }
   ],
   "source": [
    "# Launch browser (Chrome)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open the form page\n",
    "driver.get(\"https://www.selenium.dev/selenium/web/web-form.html\")\n",
    "\n",
    "# Wait for the form to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"form\")))\n",
    "\n",
    "# Fill out text fields\n",
    "driver.find_element(By.NAME, \"my-text\").send_keys(\"Luke Tam\")\n",
    "driver.find_element(By.NAME, \"my-password\").send_keys(\"TestPassword123!\")\n",
    "driver.find_element(By.NAME, \"my-textarea\").send_keys(\"This is a test message.\")\n",
    "driver.find_element(By.NAME, \"my-datalist\").send_keys(\"Boston\")\n",
    "\n",
    "# Select from dropdown\n",
    "select = Select(driver.find_element(By.NAME, \"my-select\"))\n",
    "select.select_by_visible_text(\"Two\")\n",
    "\n",
    "# Select a file\n",
    "file_path = os.path.abspath(\"Data/3 - Mid-Term Instructions.pdf\")\n",
    "driver.find_element(By.NAME, \"my-file\").send_keys(file_path)\n",
    "\n",
    "# Unselect a checkbox\n",
    "checked_checkbox = driver.find_element(By.ID, \"my-check-1\")\n",
    "if checked_checkbox.is_selected():\n",
    "    checked_checkbox.click()\n",
    "\n",
    "# Select a checkbox\n",
    "default_checkbox = driver.find_element(By.ID, \"my-check-2\")\n",
    "if not default_checkbox.is_selected():\n",
    "    default_checkbox.click()\n",
    "\n",
    "# Select a radio button\n",
    "default_radio = driver.find_element(By.ID, \"my-radio-2\")\n",
    "if not default_radio.is_selected():\n",
    "    default_radio.click()\n",
    "\n",
    "# Select a color\n",
    "driver.find_element(By.NAME, \"my-colors\").send_keys(\"#008000\")\n",
    "\n",
    "# Select a date\n",
    "driver.find_element(By.NAME, \"my-date\").send_keys(\"04/01/2025\")\n",
    "driver.find_element(By.NAME, \"my-date\").send_keys(Keys.TAB)  # hide the calendar popup\n",
    "\n",
    "# Set slider value using JavaScript\n",
    "slider = driver.find_element(By.NAME, \"my-range\")\n",
    "driver.execute_script(\"arguments[0].value = arguments[1];\", slider, \"8\")\n",
    "\n",
    "# Take a screenshot of the filled form\n",
    "driver.save_screenshot(\"Screenshots/3 - Web Form Completed.png\")\n",
    "\n",
    "# Submit the form\n",
    "driver.find_element(By.TAG_NAME, \"button\").click()\n",
    "\n",
    "# Take a screenshot of the confirmation page\n",
    "time.sleep(2)\n",
    "driver.save_screenshot(\"Screenshots/3 - Web Form Confirmation.png\")\n",
    "\n",
    "# Display success message\n",
    "print('âœ… Success! Screenshots of the completed web form and the confirmation page have been saved in the \"Screenshots\" folder.')\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362fb01-355e-4d88-a02a-54d08f1e7001",
   "metadata": {},
   "source": [
    "### Use Case 4: Web Scraping - Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e1656-745b-4bc2-aef1-533521e6abf0",
   "metadata": {},
   "source": [
    "This use case introduces basic web scraping with Selenium by extracting news headlines from the NPR News website. The script launches a browser, navigates to the NPR News section, waits for the page content to load, and collects the text of the first 10 headline links using CSS selectors. It demonstrates fundamental scraping techniques such as locating elements by tag and class, handling dynamic page loading with explicit waits, and iterating over results to extract and display clean text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18f9c2b0-631c-472d-abe3-cc10a08002e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Headlines on NPR News:\n",
      "\n",
      "1. Why gold prices are surging to record highs\n",
      "2. 2 mothers bring the House to a halt over push to allow proxy voting for new parents\n",
      "3. Top scientists warn that Trump policies are causing a 'climate of fear' in research\n",
      "4. Trump administration admits Maryland man sent to El Salvador prison by mistake\n",
      "5. Widespread firings start at federal health agencies including many in leadership\n",
      "6. What kind of support is the U.S. offering in the wake of the Myanmar quake?\n",
      "7. Thyme for some healing soup recipes from around the world\n",
      "8. Cory Booker's anti-Trump speech on the Senate floor has lasted 19 hours and counting\n",
      "9. Caregiving can test you, body and soul. It can also unlock a new sense of self\n",
      "10. Crumbling trust in American institutions: A MAHA activist takes on Girl Scout cookies\n"
     ]
    }
   ],
   "source": [
    "# Launch browser (Chrome)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Load the NPR News section\n",
    "driver.get(\"https://www.npr.org/sections/news/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# Wait until headline links are loaded\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h2.title a\")))\n",
    "\n",
    "# Grab all headline elements\n",
    "headline_elements = driver.find_elements(By.CSS_SELECTOR, \"h2.title a\")\n",
    "\n",
    "# Extract and print their text content\n",
    "print(\"Top 10 Headlines on NPR News:\\n\")\n",
    "for i, element in enumerate(headline_elements[:10], start=1):\n",
    "    print(f\"{i}. {element.text}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb5fa9-2bc7-477c-ad5c-25359d275caa",
   "metadata": {},
   "source": [
    "### Use Case 5: Web Scraping - Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f7681-1652-40ba-9343-0e04375c9635",
   "metadata": {},
   "source": [
    "This use case demonstrates intermediate-level web scraping using Selenium by extracting structured product data from OpenFoodFacts.org. The script loads the main product listing page, waits for the content to finish loading, and collects the name, URL, and nutritional labels (Nutri-Score, NOVA group, and Green-Score) for the first 10 food products. It showcases how to work with nested HTML elements, extract image metadata for additional context, and apply conditional logic to parse available product information. This example strengthens your understanding of navigating real-world HTML structures and processing mixed content types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23e21e20-c7de-4c84-8c8d-b8faf8f0ee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Products on OpenFoodFacts.org:\n",
      "\n",
      "1. Sidi Ali - 33 cl\n",
      "   Link: https://world.openfoodfacts.org/product/6111035000430/sidi-ali\n",
      "   Nutri-Score: A - Very good nutritional quality\n",
      "   NOVA Group: Unprocessed or minimally processed foods\n",
      "   Green-Score: Not applicable - Not yet applicable for the category: Waters\n",
      "\n",
      "2. Sample Product - Jaouda - 80.0\n",
      "   Link: https://world.openfoodfacts.org/product/6111242100992/sample-product-jaouda\n",
      "   Nutri-Score: Unknown - Missing data to compute the Nutri-Score\n",
      "   NOVA Group: Processed foods\n",
      "   Green-Score: B - Low environmental impact\n",
      "\n",
      "3. Sidi Ali - 2 L\n",
      "   Link: https://world.openfoodfacts.org/product/6111035002175/sidi-ali\n",
      "   Nutri-Score: A - Very good nutritional quality\n",
      "   NOVA Group: Not computed - Food processing level unknown\n",
      "   Green-Score: Not applicable - Not yet applicable for the category: Waters\n",
      "\n",
      "4. Sidi Ali mineral water - 1,5 L\n",
      "   Link: https://world.openfoodfacts.org/product/6111035000058/sidi-ali-mineral-water\n",
      "   Nutri-Score: A - Very good nutritional quality\n",
      "   NOVA Group: Unprocessed or minimally processed foods\n",
      "   Green-Score: Not applicable - Not yet applicable for the category: Waters\n",
      "\n",
      "5. Aquafina 33cl - pepsi\n",
      "   Link: https://world.openfoodfacts.org/product/6111252421568/aquafina-33cl-pepsi\n",
      "   Nutri-Score: A - Very good nutritional quality\n",
      "   NOVA Group: Not computed - Food processing level unknown\n",
      "   Green-Score: Not applicable - Not yet applicable for the category: Waters\n",
      "\n",
      "6. Lait - Jaouda - 500 mL\n",
      "   Link: https://world.openfoodfacts.org/product/6111266962187/lait-jaouda\n",
      "   Nutri-Score: B - Good nutritional quality\n",
      "   NOVA Group: Not computed - Food processing level unknown\n",
      "   Green-Score: D - High environmental impact\n",
      "\n",
      "7. CRISTALINE Eau De Source 1.5L - 1500 ml\n",
      "   Link: https://world.openfoodfacts.org/product/3274080005003/cristaline-eau-de-source-1-5l\n",
      "   Nutri-Score: A - Very good nutritional quality\n",
      "   NOVA Group: Unprocessed or minimally processed foods\n",
      "   Green-Score: Not applicable - Not yet applicable for the category: Waters\n",
      "\n",
      "8. Fromage blanc nature - MILKY FOOD PROFESSIONAL - 1kg\n",
      "   Link: https://world.openfoodfacts.org/product/6111246721261/fromage-blanc-nature-milky-food-professional\n",
      "   Nutri-Score: A - Very good nutritional quality\n",
      "   NOVA Group: Processed foods\n",
      "   Green-Score: A - Very low environmental impact\n",
      "\n",
      "9. Lait entier UHT - Jaouda - 1L\n",
      "   Link: https://world.openfoodfacts.org/product/6111242101180/lait-entier-uht-jaouda\n",
      "   Nutri-Score: B - Good nutritional quality\n",
      "   NOVA Group: Unprocessed or minimally processed foods\n",
      "   Green-Score: D - High environmental impact\n",
      "\n",
      "10. Gel corps et cheveux hypoallergÃ©nique - Cadum\n",
      "   Link: https://world.openfoodfacts.org/product/3600551125220/gel-corps-et-cheveux-hypoallergenique-cadum\n",
      "   Nutri-Score: Unknown - Missing data to compute the Nutri-Score\n",
      "   NOVA Group: Not computed - Food processing level unknown\n",
      "   Green-Score: Not computed - Unknown environmental impact\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch browser (Chrome)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open OpenFoodFacts.org\n",
    "driver.get(\"https://world.openfoodfacts.org/\")\n",
    "\n",
    "# Wait until products finish loading\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#products_match_all li\")))\n",
    "\n",
    "# Get list of products\n",
    "products = driver.find_elements(By.CSS_SELECTOR, \"#products_match_all li\")\n",
    "\n",
    "# Scrape first 10 items\n",
    "print(\"First 10 Products on OpenFoodFacts.org:\\n\")\n",
    "for i, item in enumerate(products[:10], start=1):\n",
    "\n",
    "    # Get name and link\n",
    "    link_tag = item.find_element(By.TAG_NAME, \"a\")\n",
    "    name = link_tag.text.strip()\n",
    "    link = link_tag.get_attribute(\"href\")\n",
    "\n",
    "    # Get all images and parse score info\n",
    "    images = item.find_elements(By.TAG_NAME, \"img\")\n",
    "    nutri_score = nova = green_score = \"N/A\"\n",
    "    for img in images:\n",
    "        src = img.get_attribute(\"src\")\n",
    "        title = img.get_attribute(\"title\")\n",
    "        if src:\n",
    "            if \"nutriscore\" in src:\n",
    "                nutri_score = (title.replace(\"Nutri-Score \", \"\")\n",
    "                               .replace(\"unknown\", \"Unknown\"))\n",
    "            elif \"nova-group\" in src:\n",
    "                nova = (title.replace(\"NOVA \", \"\")\n",
    "                        .replace(\"not computed\", \"Not computed\"))\n",
    "            elif \"green-score\" in src:\n",
    "                green_score = (title.replace(\"Green-Score \", \"\")\n",
    "                               .replace(\"not applicable\", \"Not applicable\")\n",
    "                               .replace(\"not computed\", \"Not computed\"))\n",
    "\n",
    "    # Display product info\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   Link: {link}\")\n",
    "    print(f\"   Nutri-Score: {nutri_score}\")\n",
    "    print(f\"   NOVA Group: {nova}\")\n",
    "    print(f\"   Green-Score: {green_score}\")\n",
    "    print()\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b35a8-2737-4059-bc3d-df3306a6af0c",
   "metadata": {},
   "source": [
    "### Use Case 6: Web Scraping - Intermediate with Infinite Scrolling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f0d59-a242-449c-9aca-a8f0945df43e",
   "metadata": {},
   "source": [
    "This use case demonstrates how to use Selenium to scrape dynamically loaded content from a webpage with infinite scrolling. The script navigates to the Quotes to Scrape site, scrolls to the bottom repeatedly until all content is loaded, and then extracts the text, author, and tags of each quote. This example introduces a common scrolling automation pattern using JavaScript and shows how to detect when no new content is being added. It's a practical example for handling social feeds, product catalogs, or any website that loads additional items as the user scrolls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18025690-51f2-44fa-99d2-932204c482b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total quotes loaded: 100\n",
      "\n",
      "1. â€œThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.â€\n",
      "   â€” Albert Einstein\n",
      "   Tags: change, deep-thoughts, thinking, world\n",
      "\n",
      "2. â€œIt is our choices, Harry, that show what we truly are, far more than our abilities.â€\n",
      "   â€” J.K. Rowling\n",
      "   Tags: abilities, choices\n",
      "\n",
      "3. â€œThere are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.â€\n",
      "   â€” Albert Einstein\n",
      "   Tags: inspirational, life, live, miracle, miracles\n",
      "\n",
      "4. â€œThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.â€\n",
      "   â€” Jane Austen\n",
      "   Tags: aliteracy, books, classic, humor\n",
      "\n",
      "5. â€œImperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.â€\n",
      "   â€” Marilyn Monroe\n",
      "   Tags: be-yourself, inspirational\n",
      "\n",
      "6. â€œTry not to become a man of success. Rather become a man of value.â€\n",
      "   â€” Albert Einstein\n",
      "   Tags: adulthood, success, value\n",
      "\n",
      "7. â€œIt is better to be hated for what you are than to be loved for what you are not.â€\n",
      "   â€” AndrÃ© Gide\n",
      "   Tags: life, love\n",
      "\n",
      "8. â€œI have not failed. I've just found 10,000 ways that won't work.â€\n",
      "   â€” Thomas A. Edison\n",
      "   Tags: edison, failure, inspirational, paraphrased\n",
      "\n",
      "9. â€œA woman is like a tea bag; you never know how strong it is until it's in hot water.â€\n",
      "   â€” Eleanor Roosevelt\n",
      "   Tags: misattributed-eleanor-roosevelt\n",
      "\n",
      "10. â€œA day without sunshine is like, you know, night.â€\n",
      "   â€” Steve Martin\n",
      "   Tags: humor, obvious, simile\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch browser (Chrome)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open the quotes page\n",
    "driver.get(\"https://quotes.toscrape.com/scroll\")\n",
    "\n",
    "# Wait for initial content\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"quote\")))\n",
    "\n",
    "# Scroll loop\n",
    "SCROLL_PAUSE_TIME = 1\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll to bottom of page and wait briefly for new content to load\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break  # No more new content loaded\n",
    "    last_height = new_height\n",
    "\n",
    "# Get all quotes\n",
    "quotes = driver.find_elements(By.CLASS_NAME, \"quote\")\n",
    "\n",
    "print(f\"Total quotes loaded: {len(quotes)}\\n\")\n",
    "\n",
    "# Display first 10 quotes\n",
    "for i, quote in enumerate(quotes[:10], start=1):\n",
    "    text = quote.find_element(By.CLASS_NAME, \"text\").text\n",
    "    author = quote.find_element(By.CLASS_NAME, \"author\").text\n",
    "    tags = [tag.text for tag in quote.find_elements(By.CLASS_NAME, \"tag\")]\n",
    "    print(f\"{i}. {text}\")\n",
    "    print(f\"   â€” {author}\")\n",
    "    print(f\"   Tags: {', '.join(tags)}\\n\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf338f-b940-412f-bc8b-3d9a4f917fd5",
   "metadata": {},
   "source": [
    "### Use Case 7: Web Scraping - Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120483d2-dbd4-460c-8965-1c4bf4d9ea91",
   "metadata": {},
   "source": [
    "This use case demonstrates advanced web scraping techniques by collecting job postings from Indeed using a live Chrome session with remote debugging. The script navigates through the first three pages of job search results for Business Analyst positions in Boston and extracts detailed information for the first two jobs on each page. For each job, it captures the job title, company, location, URL, and full job description, then saves the structured data to an Excel file. This example highlights multi-page scraping, browser tab management, dynamic content handling with explicit waits, and the use of remote browser control to bypass bot detection and CAPTCHA restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62799731-05dc-4c18-9c6a-db15016cd0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! The job details have been saved in the \"Data\" folder.\n"
     ]
    }
   ],
   "source": [
    "# ============================== #\n",
    "# INSTRUCTIONS BEFORE RUNNING THIS SCRIPT\n",
    "# ============================== #\n",
    "#\n",
    "# STEP 1: Manually start Google Chrome with remote debugging enabled.\n",
    "#         This allows Selenium to attach to your existing browser session.\n",
    "#\n",
    "# WINDOWS:\n",
    "# Open Command Prompt and run (include all quotes):\n",
    "# \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9222 --user-data-dir=\"C:\\chrome_temp\"\n",
    "#\n",
    "# macOS:\n",
    "# Open Terminal and run (include all quotes):\n",
    "# /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222 --user-data-dir=\"/tmp/chrome_temp\"\n",
    "#\n",
    "# STEP 2: Run the following Python script.\n",
    "\n",
    "# Configure Chrome to attach to the existing session\n",
    "options = Options()\n",
    "options.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9222\")\n",
    "\n",
    "# Connect to the existing Chrome session\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Open the Indeed job search page (adjust filters in URL as needed)\n",
    "# Job title: Business Analyst\n",
    "# Location: Boston, MA\n",
    "# Date posted: Last 7 days\n",
    "base_url = \"https://www.indeed.com/jobs?q=business+analyst&l=Boston%2C+MA&fromage=7&start={}\"\n",
    "results = []\n",
    "\n",
    "# Loop through first 3 pages (0, 10, 20)\n",
    "for page in range(0, 30, 10):\n",
    "    driver.get(base_url.format(page))\n",
    "    driver.refresh()\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Wait for all job cards on the page to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"job_seen_beacon\")))\n",
    "\n",
    "    # Get all job cards on the page\n",
    "    job_cards = driver.find_elements(By.CLASS_NAME, \"job_seen_beacon\")\n",
    "\n",
    "    # Loop through first 2 jobs on the page\n",
    "    for card in job_cards[:2]:\n",
    "        try:\n",
    "            # Extract job title, company, and location\n",
    "            title = card.find_element(By.TAG_NAME, \"h2\").text.strip()\n",
    "            company = card.find_element(By.CSS_SELECTOR, 'span[data-testid=\"company-name\"]').text.strip()\n",
    "            location = card.find_element(By.CSS_SELECTOR, 'div[data-testid=\"text-location\"]').text.strip()\n",
    "\n",
    "            # Build full job URL to get job description\n",
    "            link = card.find_element(By.TAG_NAME, \"a\")\n",
    "            href = link.get_attribute(\"href\")\n",
    "            job_url = href if href.startswith(\"http\") else \"https://www.indeed.com\" + href\n",
    "\n",
    "            # Open job details page in new tab and wait for page to laod\n",
    "            driver.execute_script(\"window.open(arguments[0]);\", job_url)\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Extract job description\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"jobDescriptionText\")))\n",
    "            description = driver.find_element(By.ID, \"jobDescriptionText\").text.strip()\n",
    "\n",
    "            # Save results as list of dictionraries\n",
    "            results.append({\n",
    "                \"Title\": title,\n",
    "                \"Company\": company,\n",
    "                \"Location\": location,\n",
    "                \"URL\": job_url,\n",
    "                \"Description\": description\n",
    "            })\n",
    "\n",
    "            # Close job details tab and switch back to search window\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Make sure the destination folder exists\n",
    "os.makedirs(\"Data\", exist_ok=True)\n",
    "\n",
    "# Convert list of job dictionaries to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "excel_filename = \"Data/7 - Indeed Job Postings.xlsx\"\n",
    "df.to_excel(excel_filename, index=False)\n",
    "\n",
    "# Display success message\n",
    "print('âœ… Success! The job details have been saved in the \"Data\" folder.')\n",
    "\n",
    "# Manually close the Chrome window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3dec2-d4d3-4bf2-9ad9-66b192a5a667",
   "metadata": {},
   "source": [
    "### Use Case 8: End-to-End Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54542969-417c-49bb-b94f-5be7416e128a",
   "metadata": {},
   "source": [
    "This use case presents a complete end-to-end automation pipeline using Selenium, pandas, matplotlib, and wordcloud. The script scrapes the first three news articles from NPR, extracts and cleans the full article text, identifies the top 10 most frequent non-stopwords for each article, and saves the results to a single Excel file. It also generates a bar chart and word cloud for each article, visualizing the most common words based on the full article content. This use case showcases the integration of web scraping, text processing, data visualization, and file output into a cohesive and reproducible automation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52706248-7570-498c-bcbe-661a99c64af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Processing article 1: Say goodbye to chain crews: The NFL will use camera technology to measure 1st downs\n",
      "ðŸ” Processing article 2: China's Global Electric Vehicle Boom\n",
      "ðŸ” Processing article 3: Why gold prices are surging to record highs\n",
      "âœ… Success! Headlines scraped. Excel saved. Bar chart and word cloud generated!\n"
     ]
    }
   ],
   "source": [
    "# one-time download of NLTK stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create output folders if they do not already exist\n",
    "os.makedirs(\"Data\", exist_ok=True)\n",
    "os.makedirs(\"Charts\", exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Open NPR News and get first 3 article links\n",
    "# ---------------------------\n",
    "# Launch browser (Chrome)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open NPR News and wait for headlines to load\n",
    "driver.get(\"https://www.npr.org/sections/news/\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Get the first 3 article links\n",
    "headline_elements = driver.find_elements(By.CSS_SELECTOR, 'h2.title a')\n",
    "articles = []\n",
    "seen = set()\n",
    "all_data = []\n",
    "\n",
    "for elem in headline_elements:\n",
    "    title = elem.text.strip()\n",
    "    url = elem.get_attribute(\"href\")\n",
    "\n",
    "    if title and url and url not in seen:\n",
    "        articles.append((title, url))\n",
    "        seen.add(url)\n",
    "\n",
    "    if len(articles) == 3:\n",
    "        break\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Process each article\n",
    "# ---------------------------\n",
    "for i, (title, url) in enumerate(articles, start=1):\n",
    "    print(f\"ðŸ” Processing article {i}: {title}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Try to extract article paragraphs\n",
    "    try:\n",
    "        paragraphs = driver.find_elements(By.CSS_SELECTOR, 'div.storytext p')\n",
    "        article_text = \" \".join([p.text.strip() for p in paragraphs])\n",
    "    except:\n",
    "        print(\"âŒ Failed to extract article body.\")\n",
    "        continue\n",
    "\n",
    "    # Clean and tokenize article text\n",
    "    text = article_text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    words = text.split()\n",
    "    filtered_words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    # Count top 10 words\n",
    "    word_freq = Counter(filtered_words)\n",
    "    top_words = word_freq.most_common(10)\n",
    "\n",
    "    # Store article details and word frequencies\n",
    "    for word, count in top_words:\n",
    "        all_data.append({\n",
    "            \"Article #\": i,\n",
    "            \"Headline\": title,\n",
    "            \"URL\": url,\n",
    "            \"Word\": word,\n",
    "            \"Frequency\": count\n",
    "        })\n",
    "\n",
    "    # ---------------------------\n",
    "    # Step 3: Create bar chart\n",
    "    # ---------------------------\n",
    "    words, counts = zip(*top_words)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(words, counts, color=\"green\")\n",
    "    plt.title(f\"Top Words in Article {i}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.gca().yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    plt.tight_layout()\n",
    "    bar_filename = f\"Charts/8 - Article {i} Top 10 Words.png\"\n",
    "    plt.savefig(bar_filename)\n",
    "    plt.close()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Step 4: Create word cloud\n",
    "    # ---------------------------\n",
    "    wc = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(filtered_words))\n",
    "    wc_filename = f\"Charts/8 - Article {i} Word Cloud.png\"\n",
    "    wc.to_file(wc_filename)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 5: Save all collected data to one Excel file\n",
    "# ---------------------------\n",
    "df_all = pd.DataFrame(all_data)\n",
    "excel_filename = \"Data/8 - NPR News Top 10 Words.xlsx\"\n",
    "df_all.to_excel(excel_filename, index=False)\n",
    "\n",
    "# Display success message\n",
    "print(\"âœ… Success! Headlines scraped. Excel saved. Bar charts and word clouds generated!\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
